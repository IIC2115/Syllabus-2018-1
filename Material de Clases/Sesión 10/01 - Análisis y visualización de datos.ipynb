{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC2115 - Programación como herramienta para la ingeniería</font><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y visualización de datos (aka Data Science)\n",
    "\n",
    "El análisis y visualización de datos (o ciencia de datos) es una combinación multidisciplinaria de inferencia de datos, desarrollo de algoritmos y tecnología para resolver problemas analíticamente complejos.\n",
    "\n",
    "A diferencia de otras disciplinas, en el centro de esta están los datos, por lo general grandes volúmenes de información sin procesar, transmitida y almacenada en bases de datos de distintos tipos. El objetivo principal se centra en extraer información valiosa de los datos, _buceando_ en un nivel granular para extraer y comprender comportamientos complejos, tendencias e inferencias. Esencialmente, se trata de revelar información oculta que pueda ayudar a tomar decisiones y tomar acciones inteligentes, como las siguientes:\n",
    "\n",
    "* Netflix analiza en sus datos los patrones de visualización de películas para comprender qué impulsa el interés del usuario y lo usa para tomar decisiones sobre qué serie original de Netflix producir.\n",
    "* Target (Retail) identifica cuáles son los principales segmentos de clientes dentro de su base y los comportamientos únicos de compra dentro de esos segmentos, lo que ayuda a orientar la mensajería a diferentes tipos públicos.\n",
    "* Procter & Gamble utiliza modelos de series de tiempo para comprender más claramente la demanda futura, lo que ayuda a planificar los niveles de producción de mejor manera.\n",
    "\n",
    "¿Cómo analizan los datos los experto sara extraer información? Todo comienza con la exploración de datos. Cuando se les presenta una pregunta desafiante, los analistas se convierten en algo similar a un detective, investigando pistas y tratando de comprender el patrón o las características dentro de los datos. Esto requiere una gran dosis de creatividad analítica.\n",
    "\n",
    "Luego, según sea necesario, los analistas de datos pueden aplicar una técnica cuantitativa para profundizar aún más en la información: modelos inferenciales, análisis de segmentación, previsión de series de tiempo, experimentos de control sintético, etc. El objetivo es generar científicamente una visión general de lo que realmente dicen los datos.\n",
    "\n",
    "Esta información basada en datos es fundamental para proporcionar orientación estratégica. En este sentido, los analistas de datos actúan como consultores, guiando a las partes interesadas sobre cómo actuar ante los hallazgos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas para Data Science (en Python)\n",
    "\n",
    "A continuación se muestra una lista de bibliotecas que se utilizan regularmente para realizar cálculo científico y análisis de datos en Python.\n",
    "\n",
    "* NumPy: la característica más poderosa de NumPy es el arreglo n-dimensional. Esta biblioteca también contiene, entre otras cosas, funciones básicas de álgebra lineal, transformadas de Fourier y capacidades avanzadas de generación de números aleatorios.\n",
    "* SciPy: basado en NumPy. Es una de las bibliotecas más útiles para una gran variedad de módulos de ingeniería y ciencia de alto nivel, como transformada discreta de Fourier, álgebra lineal, optimización y matrices dispersas.\n",
    "* Matplotlib: se utiliza para generar una gran variedad de gráficos, desde histogramas hasta mapas de calor.\n",
    "* Pandas: se usa para operaciones sobre datos estructurados. Se usa ampliamente para la manipulación y preparación de datos. Pandas se agregó recientemente a Python y ha sido fundamental para impulsar el uso de Python en la comunidad de ciencia de datos.\n",
    "* Scikit Learn: basada en NumPy, SciPy y matplotlib, esta biblioteca contiene una gran cantidad de herramientas eficaces para el aprendizaje de máquina y el modelamiento estadístico, que incluyen clasificación, regresión, clustering y reducción de dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El proceso de análisis de datos\n",
    "\n",
    "Ahora que estamos familiarizados con las bibliotecas adicionales, realizaremos una revisión de la resolución de problemas de datos a través de Python. En particular, el objetivo es construir un modelo predictivo eficaz., lo que nos llevará por los siguientes 3 etapas claves:\n",
    "\n",
    "* Análisis exploratorio de datos\n",
    "* Filtrado y depuración\n",
    "* Modelamiento predictivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio\n",
    "\n",
    "Para esta etapa del proceso, utiliaremos la librería Pandas de Python. En particular, la utilizaremos para leer un conjunto de datos y realizar un análisis exploratorio.\n",
    "\n",
    "Antes de cargar los datos, vamos a entender las 2 estructuras de datos clave en Pandas: Series y DataFrames\n",
    "\n",
    "Una **Serie** se puede entender como una matriz unidimensional etiquetada/indexada. Se puede acceder a elementos individuales de esta Serie a través de estas etiquetas.\n",
    "\n",
    "Un **DataFrame** es similar a un libro de Excel: tiene nombres de columnas que hacen referencia a ellas y tiene filas, a las que se puede acceder mediante el uso de números de estas. La diferencia esencial es que los nombres de columna y los números de fila se conocen como índice de columna y fila en un DataFrame.\n",
    "\n",
    "Las Series y DataFrames forman el modelo de datos básicos para Pandas en Python. Los conjuntos de datos se leen primero en DataFrames y luego se pueden aplicar fácilmente varias operaciones (por ejemplo, agrupar por, agregación, etc.) a sus columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos para el ejemplo un set de datos de información de créditos bancarios. Para importarlo, basta con utilizar la función *read_csv()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display #para mostrar más de un elemento por celda de Jupyter\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "display(df.head(10))\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función describe () proporciona el conteo, media, desviación, mínimo, cuartíles y máximo de los datos. A continuación algunas aspectos que pueden deducirse de esta información:\n",
    "\n",
    "* LoanAmount tiene (614-592) 22 valores perdidos.\n",
    "* Loan_Amount_Term tiene (614-600) 14 valores faltantes.\n",
    "* Credit_History tiene (614-564) 50 valores perdidos.\n",
    "* También podemos observar que alrededor del 84% de los solicitantes tienen un historial crediticio (la media del campo Credit_History es 0.84).\n",
    "\n",
    "Para los valores no numéricos (por ejemplo, Property_Area, Credit_History, etc.), podemos ver la distribución de frecuencias para comprender si tienen sentido o no. La tabla de frecuencias se puede imprimir con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos mirar los valores únicos del historial de crédito. Tengan en cuenta que df['columna'] es una técnica de indexación básica para acceder a una columna particular del DataFrame. Puede ser una lista de columnas también."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis distribucional\n",
    "\n",
    "Ahora que estamos familiarizados con las características básicas de los datos, estudiemos la distribución de algunas de sus variables. Comencemos con las variables numéricas ApplicantIncome y LoanAmount, en particular, con el histograma de ApplicantIncome usando los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ApplicantIncome'].hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí observamos que hay algunos valores extremos. Esta es también la razón por la cual se requieren 50 contenedores para representar claramente la distribución.\n",
    "\n",
    "A continuación, revisaremos Box Plots para comprender las distribuciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto gráfico indica la presencia de muchos valores extremos más claramente que el histograma. Esto se puede atribuir a la disparidad de ingresos en la sociedad. Parte de esto puede ser impulsado por el hecho de que estamos viendo personas con diferentes niveles de educación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome', by = 'Education')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que no hay diferencias sustanciales entre los ingresos medios de los graduados y los no graduados. Pero hay un mayor número de graduados con ingresos muy altos, que parecen ser los valores atípicos.\n",
    "\n",
    "Ahora, veamos el histograma y el Box Plot de LoanAmount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount'].hist(bins=50)\n",
    "plt.show()\n",
    "df.boxplot(column='LoanAmount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente hay valores extremos. Con el fin de facilitar el modelamiento predictivo, tanto ApplicantIncome como LoanAmount requieren una cierta cantidad de depuración de datos (los valores extremos son difíciles de predecir). LoanAmount tiene además valores incompletos y también extremos, mientras que ApplicantIncome tiene algunos valores extremos, que exigen una comprensión más profunda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de variables categóricas\n",
    "\n",
    "Ahora que entendemos las distribuciones para ApplicantIncome y LoanIncome, comprendamos las variables categóricas en más detalles. Utilizaremos tablas dinámicas tipo Excel y tabulación cruzada. Es importante notar que aquí el estado del préstamo ha sido codificado como 1 para sí y 0 para no. Por lo tanto, la media representa la probabilidad de obtener un préstamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = df['Credit_History'].value_counts(ascending=True)\n",
    "print('Tabla de frecuencia para el historial de crédito:')\n",
    "print(temp1)\n",
    "\n",
    "temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "print('\\nProbabilidad de obtener un crédito, en base a la existencia de historial crediticio:')\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas mismas tablas se pueden mostrar como un gráfico de barras usando la biblioteca *matplotlib* con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax1 = temp1.plot(kind='bar')\n",
    "ax1.set_xlabel('Credit_History')\n",
    "ax1.set_ylabel('Count of Applicants')\n",
    "ax1.set_title(\"Applicants by Credit_History\")\n",
    "\n",
    "\n",
    "ax2 = temp2.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Credit_History')\n",
    "ax2.set_ylabel('Probability of getting loan')\n",
    "ax2.set_title(\"Probability of getting loan by credit history\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Esto muestra que las posibilidades de obtener un préstamo son ocho veces mayores si el solicitante tiene un historial crediticio válido.\n",
    "\n",
    "Alternativamente, estos dos gráficos también se pueden visualizar combinándolos en un gráfico apilado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible agregar la información de género:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp4 = pd.crosstab([df['Credit_History'],df['Gender']], df['Loan_Status'])\n",
    "temp4.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabamos de ver cómo podemos hacer un análisis exploratorio en Python usando Pandas, lo que nos entregó información relevante que será utilizada en la siguiente etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza y depuración de los datos\n",
    "\n",
    "Mientras exploramos los datos, encontramos algunos problemas en estos, que deben resolverse antes de que los datos estén listos para un buen modelo. Aquí algunos de los problemas de los que ya somos conscientes:\n",
    "\n",
    "* Faltan valores en algunas variables.\n",
    "* Al observar las distribuciones, vimos que ApplicantIncome y LoanAmount parecían contener valores extremos.\n",
    "\n",
    "Además de estos problemas con los campos numéricos, también debemos ver los campos no numéricos, es decir, Género, Área de la propiedad, Casado, Educación y Dependientes para ver, si contienen información útil o incompleta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificación de los valores faltantes\n",
    "\n",
    "Echemos un vistazo a los valores faltantes en todas las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque los valores perdidos no son muy altos en número, muchas variables los tienen y cada uno de ellos debe estimarse y agregarse en los datos. Es importante tener en cuenta que los valores perdidos pueden no ser siempre NaN o null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo completar los valores perdidos en LoanAmount?\n",
    "\n",
    "Existen numerosas formas de completar los valores faltantes del monto del préstamo, siendo el más simple el reemplazo por la media, que se puede hacer mediante el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra alternativa es construir un modelo de aprendizaje supervisado para predecir el valor faltante, en función de valores no faltantes. Esto queda como ejercicio para cada uno.\n",
    "\n",
    "Dado que, el objetivo de esta sección es resaltar los pasos en la limpieza de los datos, adoptaremos un enfoque más simple que el de aprendizaje supervizado.\n",
    "\n",
    "Primero, veamos el Box Plot para ver si existe una tendencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.boxplot(column='LoanAmount', by = ['Education','Self_Employed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible apreciar algunas variaciones en la mediana del monto del préstamo para cada grupo y esto puede usarse para llenar los valores faltantes. Pero primero, debemos asegurarnos de que cada una de las variables de Self_Employed y Education no debe tener valores perdidos. Veamos la tabla de frecuencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como aproximadamente 86% de los valores son \"No\", es seguro llenar los valores faltantes como \"No\", ya que hay una alta probabilidad de éxito. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, crearemos una tabla dinñamica, que nos proporciona valores medios para todos los grupos de valores únicos de las características Self_Employed y Education. A continuación, definimos una función, que devuelve los valores de estas celdas y la aplica para completar los valores que faltan del monto del préstamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n",
    "\n",
    "def fage(x):\n",
    "    return table.loc[x['Self_Employed'],x['Education']]\n",
    "\n",
    "df[df['LoanAmount'].isnull()].apply(fage, axis=1)\n",
    "\n",
    "df['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "table.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo tratar los valores extremos?\n",
    "\n",
    "Analicemos LoanAmount primero. Dado que los valores extremos son posibles, es posible que algunas personas soliciten préstamos de alto valor debido a necesidades específicas. Entonces, en lugar de tratarlos como valores atípicos, probemos una transformación logarítmica para anular su efecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount_log'] = np.log(df['LoanAmount'])\n",
    "df['LoanAmount_log'].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora la distribución se ve mucho más cerca de una normal (preferible para los modelos predictivos) y el efecto de los valores extremos ha disminuido significativamente.\n",
    "\n",
    "Ahora, en relación a ApplicantIncome, una intuición puede ser que algunos solicitantes tienen un ingreso más bajo, pero tiene avales fuertes. Por lo tanto, podría ser una buena idea combinar ambos ingresos como ingreso total y tomar una transformación de este valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df['TotalIncome_log'] = np.log(df['TotalIncome'])\n",
    "df['TotalIncome_log'].hist(bins=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos que la distribución es mucho mejor que antes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de modelos predictivos\n",
    "\n",
    "Ahora que los datos han sido filtrados, veamos código en Python para crear un modelos predictivos para nuestro conjunto de datos. Skicit-Learn (sklearn) es la biblioteca más utilizada en Python para este propósito. Como, sklearn requiere que todas las entradas sean numéricas, debemos convertir todas nuestras variables categóricas en numéricas codificando las categorías. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"train.csv\").dropna() # eliminamos las filas con algún valor desconocido para evitar problemas\n",
    "\n",
    "var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importaremos los módulos requeridos, luego definiremos una función de clasificación genérica, que toma un modelo como entrada y determina el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "    scores = cross_val_score(model, data[predictors], data[outcome], cv=10)\n",
    "    \n",
    "    model.fit(data[predictors],data[outcome])\n",
    "    predictions = model.predict(data[predictors])\n",
    "    accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "    \n",
    "    print(\"Rendimiento : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "    print(\"Rendimiento corregido : %s\" % \"{0:.3%}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logística\n",
    "\n",
    "Hagamos nuestro primer modelo de Regresión Logística. Una forma sería tomar todas las variables en el modelo, pero esto podría resultar en un ajuste excesivo discutiremos esto en clases). En palabras simples, tomar todas las variables puede hacer que el modelo comprenda relaciones complejas entre los datos, que realmente no existen (ruido).\n",
    "\n",
    "Podemos hacer fácilmente algunas hipótesis intuitivas para comenzar. Las posibilidades de obtener un préstamo serán mayores para:\n",
    "\n",
    "* Los solicitantes tienen un historial de crédito (lo vimos anteriromente)\n",
    "* Solicitantes con mayores ingresos\n",
    "* Solicitantes con nivel de educación superior\n",
    "* Propiedades en áreas urbanas con altas perspectivas de crecimiento\n",
    "\n",
    "Así que vamos a hacer nuestro primer modelo con 'Credit_History'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var = 'Loan_Status'\n",
    "model = LogisticRegression()\n",
    "predictor_var = ['Credit_History']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, usemos más variables en el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['Credit_History','Education','Married','Self_Employed','Property_Area']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, esperamos que el rendimiento aumente al agregar variables. Pero este es un caso más desafiante, ya que el rendimiento no se ve afectado por esto. Veamos ahora otras técnicas que podrían entregar distintos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de decisión\n",
    "\n",
    "Los árboles de decisión son otro método para construir un modelo predictivo. Generalmente proporciona un mejor rendimiento que el modelo de regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "predictor_var = ['Credit_History','Loan_Amount_Term','LoanAmount']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aquí observamos que, aunque aumentó el rendimiento, la corrección de este (validación cruzada) bajó. Este es el resultado de un modelo que sobreajusta los datos. Queda como ejercicio buscar nuevas combinaciones de variables que mejoren el rendimiento corregido (o la creación de nuevas variables, como vimos anteriormente)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
